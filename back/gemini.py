import google.generativeai as genai
from config import settings
import json
import re
from datetime import datetime, timedelta
import requests
import base64
from io import BytesIO
import uuid
import os
from google.cloud import texttospeech
from vertexai.preview.vision_models import ImageGenerationModel
import vertexai

genai.configure(api_key=settings.GEMINI_API_KEY)

# Initialize Vertex AI
try:
    vertexai.init(project=settings.GOOGLE_CLOUD_PROJECT_ID, location=settings.GOOGLE_CLOUD_LOCATION)
    print(f"Vertex AI initialized with project: {settings.GOOGLE_CLOUD_PROJECT_ID}")
except Exception as e:
    print(f"Warning: Vertex AI initialization failed: {e}")

SYSTEM_PROMPT_AIDS = """ä½ æ˜¯å°æä»è®°å¿†æ­å­ï¼Œè´Ÿè´£å¸®åŠ©ç”¨æˆ·è®°å¿†ã€‚ä½ ä¼šæ ¹æ®ç”¨æˆ·è¾“å…¥çš„å†…å®¹ï¼Œç”Ÿæˆæ€ç»´å¯¼å›¾ã€è®°å¿†å£è¯€å’Œæ„Ÿå®˜è”æƒ³ã€‚

è®°å¿†å£è¯€ç”Ÿæˆä¸‰ç§ç±»å‹ï¼šé¡ºå£æºœè®°å¿†æ³•ã€é¦–å­—æ¯è®°å¿†æ³•ã€æ•…äº‹è”æƒ³æ³•ã€‚
æ„Ÿå®˜è”æƒ³ä¹Ÿåˆ†ä¸ºä¸‰ç±»ï¼šè§†è§‰è”æƒ³ã€å¬è§‰è”æƒ³å’Œè§¦è§‰è”æƒ³ã€‚

æ³¨æ„ä¸¥æ ¼æŒ‰ç…§å¦‚ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸éœ€è¦ä»»ä½•å¤šä½™çš„å†…å®¹ï¼Œåªéœ€è¦åœ¨å¯¹åº”çš„ä½ç½®å¡«å…¥ contentï¼š

{
  "mindMap": {
    "id": "root",
    "label": "content" æˆ– "è®°å¿†ä¸»é¢˜",
    "children": [
      {
        "id": "part1",
        "label": "content",
        "children": [
          { "id": "leaf1", "label": "content" },
          { "id": "leaf2", "label": "content" }
        ]
      },
      {
        "id": "part2",
        "label": "content",
        "children": [
          { "id": "leaf3", "label": "content" },
          { "id": "leaf4", "label": "content" }
        ]
      }
    ]
  },
  "mnemonics": [
    {
      "id": "rhyme",
      "title": "é¡ºå£æºœè®°å¿†æ³•",
      "content": "contentï¼Œé¡ºå£æºœåŠ©è®°ã€‚",
      "type": "rhyme"
    },
    {
      "id": "acronym",
      "title": "é¦–å­—æ³•",
      "content": "",
      "type": "acronym",
      "explanation": "åˆ©ç”¨é¦–å­—æ¯è®°å¿†"
    },
    {
      "id": "story",
      "title": "æ•…äº‹è”æƒ³æ³•",
      "content": "æƒ³è±¡ä¸€ä¸ªæ•…äº‹ä¸²è”ï¼š",
      "type": "story"
    }
  ],
  "sensoryAssociations": [
    {
      "id": "visual",
      "title": "è§†è§‰è”æƒ³",
      "type": "visual",
      "content": [
        {
          "dynasty": "content",
          "image": "ğŸŒŸ",
          "color": "#fbbf24",
          "association": ""
        },
        {
          "dynasty": "content",
          "image": "ğŸ”µ",
          "color": "#06b6d4",
          "association": ""
        }
      ]
    },
    {
      "id": "auditory",
      "title": "å¬è§‰è”æƒ³",
      "type": "auditory",
      "content": [
        { "dynasty": "content", "sound": "å®å’šå£°", "rhythm": "èŠ‚å¥æ„Ÿ" },
        { "dynasty": "content", "sound": "é£å£°", "rhythm": "è½»å¿«" }
      ]
    },
    {
      "id": "tactile",
      "title": "è§¦è§‰è”æƒ³",
      "type": "tactile",
      "content": [
        { "dynasty": "content", "texture": "æŸ”è½¯", "feeling": "æ¸©æš–" },
        { "dynasty": "content", "texture": "åšç¡¬", "feeling": "å†°å‡‰" }
      ]
    }
  ]
}
"""

# Although we ask Gemini for the schedule, it's more reliable to calculate it in code.
# The prompt serves as a logical guide, but the implementation is deterministic.
def generate_review_schedule_from_ebbinghaus():
    """
    Generates a review schedule based on the Ebbinghaus forgetting curve.
    """
    now = datetime.now()
    review_intervals = [
        timedelta(minutes=20),
        timedelta(hours=1),
        timedelta(hours=9),
        timedelta(days=1),
        timedelta(days=2),
        timedelta(days=4),
        timedelta(days=7),
        timedelta(days=15),
    ]
    review_dates = [(now + interval).isoformat() for interval in review_intervals]
    return {"review_dates": review_dates}

def parse_gemini_response(text: str):
    try:
        # Clean the text by removing ```json and ``` markers
        cleaned_text = re.sub(r'```json\n?|```', '', text)
        return json.loads(cleaned_text)
    except Exception as e:
        print(f"Error parsing Gemini response: {e}")
        return None

async def generate_memory_aids(content: str):
    model = genai.GenerativeModel('gemini-1.5-flash')
    prompt = f"{SYSTEM_PROMPT_AIDS}\n\nç”¨æˆ·è¾“å…¥çš„å†…å®¹ï¼š{content}\n\nè¯·ä¸ºè¿™ä¸ªå†…å®¹ç”Ÿæˆè®°å¿†è¾…åŠ©å·¥å…·."

    try:
        response = await model.generate_content_async(prompt)
        # print("Gemini aids response:", response.text)
        parsed_response = parse_gemini_response(response.text)
        return parsed_response
    except Exception as e:
        print(f"Error calling Gemini API for aids: {e}")
        return None

async def generate_image(content: str, context: str = ""):
    """
    Generate an actual image based on the visual association content using Google Imagen
    """
    # First, generate a detailed prompt using Gemini
    model = genai.GenerativeModel('gemini-1.5-flash')
    prompt_generation = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒç”Ÿæˆæç¤ºè¯ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹è§†è§‰è”æƒ³å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„ã€é€‚åˆAIå›¾åƒç”Ÿæˆçš„è‹±æ–‡æç¤ºè¯ã€‚

è§†è§‰è”æƒ³å†…å®¹ï¼š{content}
ä¸Šä¸‹æ–‡ï¼š{context}

è¦æ±‚ï¼š
1. æç¤ºè¯åº”è¯¥æ˜¯è‹±æ–‡
2. æè¿°è¦å…·ä½“ã€ç”ŸåŠ¨
3. é€‚åˆè®°å¿†è¾…åŠ©çš„è§†è§‰åŒ–è¡¨è¾¾
4. é£æ ¼æ¸…æ™°ã€è‰²å½©é²œæ˜
5. åªè¿”å›æç¤ºè¯ï¼Œä¸è¦å…¶ä»–å†…å®¹

ç¤ºä¾‹æ ¼å¼ï¼š"A vibrant illustration of..."
"""

    try:
        # Generate the prompt
        prompt_response = await model.generate_content_async(prompt_generation)
        image_prompt = prompt_response.text.strip()
        
        try:
            # Try to generate the actual image using Google Imagen
            generation_model = ImageGenerationModel.from_pretrained("imagen-3.0-generate-001")
            response = generation_model.generate_images(
                prompt=image_prompt,
                number_of_images=1,
                aspect_ratio="1:1",
                add_watermark=False,
            )
            
            # Get the generated image
            generated_image = response.images[0]
            
            # Convert image to base64
            image_bytes = generated_image._image_bytes
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
            
            return {
                "image_url": None,  # Imagen doesn't provide URLs, only image data
                "image_base64": f"data:image/png;base64,{image_base64}",
                "prompt": image_prompt
            }
        except Exception as imagen_error:
            print(f"Imagen generation failed: {imagen_error}")
            # Return prompt only when image generation fails
            return {
                "image_url": None,
                "image_base64": None,
                "prompt": image_prompt  # Still return the generated prompt
            }
            
    except Exception as e:
        print(f"Error generating image prompt: {e}")
        return {
            "image_url": None,
            "image_base64": None,
            "prompt": f"å›¾åƒç”Ÿæˆæç¤ºè¯ï¼š{content}çš„è§†è§‰åŒ–è¡¨è¾¾"  # Fallback prompt
        }

async def generate_audio(content: str, context: str = ""):
    """
    Generate actual audio based on the auditory association content.
    For environmental sounds (like truck engine), generate sound effects.
    For other content, use Text-to-Speech.
    """
    # Check if the content is requesting environmental sounds/effects
    environmental_sounds = [
        "å¼•æ“", "å‘åŠ¨æœº", "é©¬è¾¾", "æœºå™¨", "é£å£°", "é›¨å£°", "æµ·æµª", "é¸Ÿå«", "è™«é¸£", 
        "é›·å£°", "é’Ÿå£°", "è­¦æŠ¥", "æ±½ç¬›", "å–‡å­", "æ•²å‡»", "æ‘©æ“¦", "æ»´æ°´", "æµæ°´",
        "è„šæ­¥", "å¿ƒè·³", "å‘¼å¸", "å’€åš¼", "æ’•çº¸", "å¼€é—¨", "å…³é—¨", "é”®ç›˜", "é¼ æ ‡"
    ]
    
    is_environmental_sound = any(sound in content for sound in environmental_sounds)
    
    if is_environmental_sound:
        # For environmental sounds, generate a synthetic sound effect description
        # and return a placeholder for actual sound generation
        model = genai.GenerativeModel('gemini-1.5-flash')
        sound_description = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å£°éŸ³æ•ˆæœæè¿°ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹å£°éŸ³å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„å£°éŸ³ç‰¹å¾æè¿°ã€‚

å£°éŸ³å†…å®¹ï¼š{content}
ä¸Šä¸‹æ–‡ï¼š{context}

è¦æ±‚ï¼š
1. æè¿°å£°éŸ³çš„é¢‘ç‡ç‰¹å¾ï¼ˆä½é¢‘ã€ä¸­é¢‘ã€é«˜é¢‘ï¼‰
2. æè¿°å£°éŸ³çš„èŠ‚å¥å’ŒæŒç»­æ€§
3. æè¿°å£°éŸ³çš„å¼ºåº¦å˜åŒ–
4. ç”¨ä¸“ä¸šæœ¯è¯­æè¿°å£°éŸ³ç‰¹å¾
5. é•¿åº¦æ§åˆ¶åœ¨100å­—ä»¥å†…

ç¤ºä¾‹æ ¼å¼ï¼š"ä½é¢‘æŒç»­è½°é¸£ï¼Œé¢‘ç‡çº¦50-200Hzï¼ŒèŠ‚å¥ç¨³å®šï¼Œå¼ºåº¦ä¸­ç­‰ï¼Œå¸¦æœ‰è½»å¾®çš„æœºæ¢°æŒ¯åŠ¨æ„Ÿ..."
"""
        
        try:
            response = await model.generate_content_async(sound_description)
            sound_spec = response.text.strip()
            
            # Return a structured response for environmental sounds
            return {
                "audio_base64": None,  # No actual audio generated yet
                "script": f"ç¯å¢ƒéŸ³æ•ˆï¼š{content}",
                "sound_description": sound_spec,
                "sound_type": "environmental",
                "duration": 5.0,  # Default 5 seconds for environmental sounds
                "message": "å·²ç”Ÿæˆå£°éŸ³ç‰¹å¾æè¿°ï¼Œå®é™…éŸ³æ•ˆç”ŸæˆåŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­"
            }
        except Exception as e:
            print(f"Error generating sound description: {e}")
            return {
                "audio_base64": None,
                "script": f"ç¯å¢ƒéŸ³æ•ˆï¼š{content}",
                "sound_description": f"è¯·æ±‚çš„å£°éŸ³ï¼š{content}ï¼Œç‰¹å¾ï¼šæŒç»­æ€§ç¯å¢ƒéŸ³æ•ˆ",
                "sound_type": "environmental",
                "duration": 5.0,
                "message": "å£°éŸ³ç‰¹å¾æè¿°ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æè¿°"
            }
    
    # For non-environmental sounds, use the original TTS approach
    model = genai.GenerativeModel('gemini-1.5-flash')
    script_generation = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éŸ³é¢‘è„šæœ¬ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¬è§‰è”æƒ³å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªé€‚åˆè¯­éŸ³åˆæˆçš„ä¸­æ–‡è„šæœ¬ã€‚

å¬è§‰è”æƒ³å†…å®¹ï¼š{content}
ä¸Šä¸‹æ–‡ï¼š{context}

è¦æ±‚ï¼š
1. ç”Ÿæˆé€‚åˆTTSæœ—è¯»çš„ä¸­æ–‡æ–‡æœ¬
2. å†…å®¹è¦æœ‰åŠ©äºè®°å¿†è”æƒ³
3. è¯­è¨€è‡ªç„¶æµç•…
4. é•¿åº¦é€‚ä¸­ï¼ˆ50-200å­—ï¼‰
5. åªè¿”å›è„šæœ¬æ–‡æœ¬ï¼Œä¸è¦å…¶ä»–å†…å®¹

ç¤ºä¾‹æ ¼å¼ï¼š"æƒ³è±¡ä¸€ä¸‹ï¼Œå½“ä½ å¬åˆ°è½»å¿«çš„é’¢ç´å£°æ—¶..."
"""

    try:
        # Generate the script
        script_response = await model.generate_content_async(script_generation)
        audio_script = script_response.text.strip()
        
        try:
            # Try to generate the actual audio using Google Text-to-Speech
            client = texttospeech.TextToSpeechClient()
            
            # Set the text input to be synthesized
            synthesis_input = texttospeech.SynthesisInput(text=audio_script)
            
            # Build the voice request, select the language code and the voice gender
            # Using Basic voice for more synthetic/robotic sound instead of natural human voice
            voice = texttospeech.VoiceSelectionParams(
                language_code="cmn-CN",
                name="cmn-CN-Standard-C",  # Using Standard-C for more synthetic sound
                ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL,  # Neutral gender for more robotic feel
            )
            
            # Select the type of audio file you want returned
            # Adding effects to make it sound more synthetic
            audio_config = texttospeech.AudioConfig(
                audio_encoding=texttospeech.AudioEncoding.MP3,
                speaking_rate=0.9,  # Slightly slower for more robotic feel
                pitch=-2.0,  # Lower pitch for synthetic sound
                volume_gain_db=-3.0  # Slightly quieter
            )
            
            # Perform the text-to-speech request
            response = client.synthesize_speech(
                input=synthesis_input, voice=voice, audio_config=audio_config
            )
            
            # Convert audio to base64
            audio_base64 = base64.b64encode(response.audio_content).decode('utf-8')
            
            return {
                "audio_base64": f"data:audio/mp3;base64,{audio_base64}",
                "script": audio_script,
                "duration": len(audio_script) * 0.1  # Rough estimate
            }
        except Exception as tts_error:
            print(f"Text-to-Speech generation failed: {tts_error}")
            # Return script only when audio generation fails
            return {
                "audio_base64": None,
                "script": audio_script,  # Still return the generated script
                "duration": len(audio_script) * 0.1
            }
        
    except Exception as e:
        print(f"Error generating audio script: {e}")
        return {
            "audio_base64": None,
            "script": f"éŸ³é¢‘è„šæœ¬ï¼š{content}çš„å¬è§‰è”æƒ³æè¿°",  # Fallback script
            "duration": 0
        }

